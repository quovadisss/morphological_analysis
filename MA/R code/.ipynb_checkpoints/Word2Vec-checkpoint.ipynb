{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from lxml import etree\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans, MeanShift, estimate_bandwidth, DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from nltk.tag import pos_tag\n",
    "from collections import Counter\n",
    "from matplotlib import cm\n",
    "\n",
    "import gensim, pickle, math\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load df\n",
    "with open('morpho_df.pkl', 'rb') as f:\n",
    "    df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # w2v using pretrained google news model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained google news w2v model\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('C:/Users/user/Desktop/Lab/박민규/text_mining/GoogleNews-vectors-negative300.bin', binary=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "814\n",
      "209\n",
      "7848\n",
      "2873\n",
      "578\n"
     ]
    }
   ],
   "source": [
    "# remove duplicates and make list for each column\n",
    "product = list(set(df['ppl_product'].dropna()))\n",
    "print(len(product))\n",
    "norp = list(set(df['ppl_norp'].dropna()))\n",
    "print(len(norp))\n",
    "org = list(set(df['ppl_org'].dropna()))\n",
    "print(len(org))\n",
    "tech = list(set(df['technology'].dropna()))\n",
    "print(len(tech))\n",
    "sys = list(set(df['system'].dropna()))\n",
    "print(len(sys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get 300 dimensions w2v for each list\n",
    "def get_w2v(x):\n",
    "    new = []\n",
    "    get_w2v_list = []\n",
    "    for i in x:\n",
    "        try:\n",
    "            w2v = model[i]\n",
    "            get_w2v_list.append(w2v)\n",
    "            new.append(i)\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return get_w2v_list, new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_w2v, product_new = get_w2v(product)\n",
    "norp_w2v, norp_new = get_w2v(norp)\n",
    "org_w2v, org_new = get_w2v(org)\n",
    "tech_w2v, tech_new = get_w2v(tech)\n",
    "sys_w2v, sys_new = get_w2v(sys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K means, word/Index dictionary\n",
    "def k_means(x, num_culsters, y):\n",
    "    # pca_result = PCA_reduction(x)\n",
    "    kmeans_clustering = KMeans(n_clusters = num_culsters)\n",
    "    idx = list(kmeans_clustering.fit_predict(x))\n",
    "    names = y\n",
    "    word_centroid_map = {names[i]: idx[i] for i in range(len(names))}\n",
    "    return word_centroid_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GMM\n",
    "def get_gmm(x, y, num_clusters):\n",
    "    gmm = GaussianMixture(n_components=num_clusters, random_state=0)\n",
    "    gmm_label = list(gmm.fit(x).predict(x))\n",
    "    words = y\n",
    "    word_centroid_map = {words[i]: gmm_label[i] for i in range(len(words))}\n",
    "    return word_centroid_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-means\n",
    "product_cluster = k_means(product_w2v, 70, product_new)\n",
    "norp_cluster = k_means(norp_w2v, 24, norp_new)\n",
    "org_cluster = k_means(org_w2v, 215, org_new)\n",
    "tech_cluster = k_means(tech_w2v, 240, tech_new)\n",
    "sys_cluster = k_means(sys_w2v, 245, sys_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gmm\n",
    "sys_gmm = get_gmm(sys_w2v, sys_new, 134)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "505 81 4031 2172 476\n"
     ]
    }
   ],
   "source": [
    "print(len(product_new), len(norp_new), len(org_new), len(tech_new), len(sys_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_df(x, y):\n",
    "    all_words = []\n",
    "    for cluster in range(0, y):\n",
    "        words = []\n",
    "        for i in range(0,len(list(x.values()))):\n",
    "            if(list(x.values())[i] == cluster):\n",
    "                words.append(list(x.keys())[i])\n",
    "        all_words.append(words)\n",
    "    number = 0\n",
    "    for i in all_words:\n",
    "        if len(i) <= 5 and len(i) >= 2:\n",
    "            number += 1\n",
    "    return all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_product = cluster_df(product_cluster, 70)\n",
    "final_norp = cluster_df(norp_cluster, 24)\n",
    "final_org = cluster_df(org_cluster, 215)\n",
    "final_tech = cluster_df(tech_cluster, 240)\n",
    "final_sys = cluster_df(sys_cluster, 245)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anderson', 'watson', 'mario', 'johnson', 'fda'] \n",
      "\n",
      "['analysis', 'study', 'investigation', 'data', 'report', 'review', 'audit'] \n",
      "\n",
      "['vulnerability', 'flaw'] \n",
      "\n",
      "['partnership', 'collaboration'] \n",
      "\n",
      "['trojan', 'spyware', 'virus'] \n",
      "\n",
      "['hack', 'hacker', 'hacking'] \n",
      "\n",
      "['view', 'viewpoint', 'perspective'] \n",
      "\n",
      "['market', 'subsectors', 'sector'] \n",
      "\n",
      "['apple', 'orchard'] \n",
      "\n",
      "['doctor', 'surgeon', 'radiologist'] \n",
      "\n",
      "['analytics', 'optimization'] \n",
      "\n",
      "['imaging', 'visualization', 'workflow'] \n",
      "\n",
      "['ceo', 'cio', 'svp', 'cfo'] \n",
      "\n",
      "['mathematics', 'science'] \n",
      "\n",
      "['cardiology', 'radiology'] \n",
      "\n",
      "['medicare', 'health', 'heath', 'healthcare'] \n",
      "\n",
      "['million', 'rate', 'percent'] \n",
      "\n",
      "['enigma', 'task', 'puzzle', 'challenge', 'conundrum'] \n",
      "\n",
      "['xml', 'sql', 'ibm', 'api'] \n",
      "\n",
      "['terabyte', 'gigabyte'] \n",
      "\n",
      "['acuity', 'nurse', 'clinician', 'care', 'hospital', 'patient', 'medicine'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "again15_30 = []\n",
    "again31_40 = []\n",
    "again40_50 = []\n",
    "for i in final_product:\n",
    "    if len(i) <= 15 and len(i) > 1:\n",
    "        print(i, '\\n')\n",
    "    elif len(i) > 15 and len(i) <= 30:\n",
    "        again15_30.append(i)\n",
    "    elif len(i) > 30 and len(i) <= 40:\n",
    "        again31_40.append(i)\n",
    "    elif len(i) > 40 and len(i) <= 50:\n",
    "        again40_50.append(i)\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 0 0\n"
     ]
    }
   ],
   "source": [
    "print(len(again15_30), len(again31_40), len(again40_50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_others(x, number):\n",
    "    for i in x:\n",
    "        again15_w2v, again15_word = get_w2v(i)\n",
    "        again15_cluster = k_means(again15_w2v, number, again15_word)\n",
    "        again15_final = cluster_df(again15_cluster, number)\n",
    "        print('len of clusters:', len(again15_final))\n",
    "        for i in again15_final:\n",
    "            if len(i) > 1:\n",
    "                print(i, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of clusters: 3\n",
      "['ipad', 'samsung'] \n",
      "\n",
      "['opensource', 'nvidia', 'iso', 'encryption', 'server', 'java', 'apache', 'google', 'mozilla', 'ftp', 'amazon', 'http', 'vpn'] \n",
      "\n",
      "['wifi', 'voip'] \n",
      "\n",
      "len of clusters: 3\n",
      "['lea', 'cole', 'burton', 'sullivan', 'cro', 'anne', 'jane', 'tracy', 'europe', 'boston', 'philip', 'harvard', 'robert', 'john', 'bradley', 'murray'] \n",
      "\n",
      "len of clusters: 3\n",
      "['centricity', 'innovation', 'reshaping', 'initiative', 'growth', 'engagement'] \n",
      "\n",
      "['build', 'procurement', 'transformation', 'project', 'integration', 'mobilization', 'deployment', 'development', 'infrastructure', 'discovery'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_others(again15_30, 3)\n",
    "print_others(again31_40, 4)\n",
    "print_others(again40_50, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N of 1: 43\n",
      "N of clusters : 180\n",
      "N of 1: 49\n",
      "N of clusters : 185\n",
      "N of 1: 47\n",
      "N of clusters : 190\n",
      "N of 1: 50\n",
      "N of clusters : 195\n",
      "N of 1: 53\n",
      "N of clusters : 200\n",
      "N of 1: 51\n",
      "N of clusters : 205\n",
      "N of 1: 50\n",
      "N of clusters : 210\n",
      "N of 1: 46\n",
      "N of clusters : 215\n",
      "N of 1: 49\n",
      "N of clusters : 220\n",
      "N of 1: 50\n",
      "N of clusters : 225\n",
      "N of 1: 53\n",
      "N of clusters : 230\n",
      "N of 1: 49\n",
      "N of clusters : 235\n",
      "N of 1: 52\n",
      "N of clusters : 240\n",
      "N of 1: 61\n",
      "N of clusters : 245\n"
     ]
    }
   ],
   "source": [
    "for i in range(180, 250, 5):\n",
    "    test_cluster = k_means(sys_w2v, i, sys_new)\n",
    "    # print('N of clusters :', i)\n",
    "    number = cluster_df(test_cluster, i)\n",
    "    if number >= 10:\n",
    "        print('N of 1:', number)\n",
    "        print('N of clusters :', i)\n",
    "    # print('Ratio:', np.round(number/i, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show words by each cluster\n",
    "def show_words(x):\n",
    "    for cluster in range(0,10):\n",
    "        # cluster number\n",
    "        print(\"\\nCluster {}\".format(cluster))\n",
    "\n",
    "        # words\n",
    "        words = []\n",
    "        for i in range(0,len(list(x.values()))):\n",
    "            if( list(x.values())[i] == cluster ):\n",
    "                words.append(list(x.keys())[i])\n",
    "        print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
