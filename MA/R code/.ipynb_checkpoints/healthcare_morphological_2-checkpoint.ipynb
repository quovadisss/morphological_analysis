{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Morphological Analysis 2 <br>\n",
    "This 2nd analysis is to minimize number of words in senario table. Because if there are too many words in it, possible senarios are huge. So it's hard to find meaningful senarios for NSD(New Service Developement). <br>\n",
    "So I have 3 options to minimize number of words. <br>\n",
    "1. Counter <br>\n",
    "- Small amounts of counted words can be deleted because it is considered as not that important. <br><br>\n",
    "2. TF-IDF <br>\n",
    "- TF-IDF helps to find words that not important in each documents. <br><br>\n",
    "3. Co-occurrence <br>\n",
    "- By calculating co-occurrence between each words, centralities for words would be found and can get rid of words which has small centrality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import pos_tag\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import re, string, itertools, pickle\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF for 'People'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read\n",
    "news_1 = pd.read_excel(r'C:\\Users\\AMD3600\\git\\Lab\\healthcaremarket.xlsx')\n",
    "news_2 = pd.read_excel(r'C:\\Users\\AMD3600\\git\\Lab\\healthcareitnews_add.xlsx')\n",
    "news_1 = news_1.iloc[447:,:]\n",
    "news_1 = news_1['text']\n",
    "news_2 = news_2['text']\n",
    "news = pd.concat([news_1, news_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "def pre_process_wordlist(txt):\n",
    "    txt = txt.strip()\n",
    "    txt = txt.lower()\n",
    "    txt = txt.replace('\\t', ' ')\n",
    "    txt = txt.replace('\\n', '')\n",
    "    txt = txt.replace('.', ' ')\n",
    "    #txt = re.sub('[^a-zA-Z0-9]',' ',txt)\n",
    "    txt = re.sub('[$]','',txt)\n",
    "    txt = re.sub('[–]','',txt)\n",
    "    txt = re.sub('[0-9]',' ',txt)\n",
    "    txt = txt.replace(u'\\xa0', u' ')\n",
    "    txt = txt.replace('  ', ' ')\n",
    "    txt = txt.replace('   ', ' ')\n",
    "    txt = txt.replace('  ', ' ')\n",
    "    # txt = txt.replace(' .', '.')\n",
    "    txt = re.sub('[-=+,#/\\?:“”^\"—$€£@*\\\"※~&%ㆍ!』’\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》]', '', txt)\n",
    "    txt = txt.strip()\n",
    "    # Delete 1-2 length words\n",
    "    #shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')\n",
    "    #txt = shortword.sub('', txt)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "preresults = news.apply(lambda x:pre_process_wordlist(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Noun Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize noun to make clear to count noun words\n",
    "def lemm_noun(x):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    lemm_list = []\n",
    "    \n",
    "    tokens = word_tokenize(x)\n",
    "    pos_words = pos_tag(tokens)\n",
    "    \n",
    "    for word, pos in pos_words:\n",
    "        if pos[:2] == 'NN':\n",
    "            lemm_list.append(lemmatizer.lemmatize(word, pos='n'))\n",
    "        else:\n",
    "            lemm_list.append(word)\n",
    "    \n",
    "    result = ' '.join(lemm_list)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_results = preresults.apply(lambda x:lemm_noun(x)).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Get TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf_dict(x):\n",
    "    cv = CountVectorizer()\n",
    "    noun_bow = cv.fit_transform(x)\n",
    "    transformer = TfidfTransformer()\n",
    "    tfidf_matrix = transformer.fit_transform(noun_bow)\n",
    "    \n",
    "    # create dictionary to find a tfidf word each word\n",
    "    word2tfidf = dict(zip(cv.get_feature_names(), transformer.idf_))\n",
    "        \n",
    "    return word2tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_dict = tf_idf_dict(noun_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Find words that indicate 'People'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract only noun with stopwords and get people with NER\n",
    "def people_noun(x):\n",
    "    stop = stopwords.words('english')\n",
    "    is_noun = lambda pos: pos[:2] == 'NN'\n",
    "    \n",
    "    noun_list = []\n",
    "    for doc in x:\n",
    "        tokens = word_tokenize(doc)\n",
    "        noun = [word for (word, pos) in pos_tag(tokens) if is_noun(pos)]\n",
    "        noun_list.extend(noun)\n",
    "    \n",
    "    result = [i for i in noun_list if not i in stop]\n",
    "\n",
    "    # split corpus due to the fact that nlp's limit is 1,000,000\n",
    "    all_corpus = []\n",
    "    for i in range(0, len(result), 8000):\n",
    "        corpus = ' '.join(result[i:i+8000])\n",
    "        all_corpus.append(corpus)\n",
    "        \n",
    "    # NER dictionary in spacy\n",
    "    nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "    ner_dict = {}\n",
    "    for i in all_corpus:\n",
    "        doc = nlp(i)\n",
    "        for e in doc:\n",
    "            if e.ent_type_ != \"\":\n",
    "                if len(e) > 2:\n",
    "                    ner_dict[e] = e.ent_type_\n",
    "    \n",
    "    # get words by each NER\n",
    "    norp = []\n",
    "    org = []\n",
    "    product = []\n",
    "    for i, j in ner_dict.items():\n",
    "        if j == 'NORP':\n",
    "            norp.append(str(i))\n",
    "        elif j == 'ORG':\n",
    "            org.append(str(i))\n",
    "        elif j == 'PRODUCT':\n",
    "            product.append(str(i))\n",
    "            \n",
    "    return norp, org, product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "norp, org, product = people_noun(noun_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59112"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(org)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Make Dataframe words and tf-idf scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tfidf_dataframe(dic, filt):\n",
    "    words = []\n",
    "    scores = []\n",
    "    for word, score in dic.items():\n",
    "        if len(word) > \n",
    "        if word in filt:\n",
    "            words.append(word)\n",
    "            scores.append(score)\n",
    "    words = pd.Series(words)\n",
    "    scores = pd.Series(scores)\n",
    "    tfidf_df = pd.DataFrame({'words' : words,\n",
    "                             'tfidf' : scores}).sort_values(by=['tfidf'])\n",
    "    \n",
    "    return tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df = make_tfidf_dataframe(tfidf_dict, org)\n",
    "with open('people_tfidf_df.pkl', 'wb') as f:\n",
    "    pickle.dump(tfidf_df, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF for 'Tech'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. get 'Tech' words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract two words after a specific word\n",
    "def get_index(doc, cword):\n",
    "    tokens = word_tokenize(doc)\n",
    "    words_index = []\n",
    "    for i, j in enumerate(tokens):\n",
    "        if j == cword:\n",
    "            word_index = [i+1, i+2]\n",
    "            words_index.append(word_index)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    words = []\n",
    "    for two_index in words_index:\n",
    "        word = []\n",
    "        for i, j in enumerate(tokens):\n",
    "            if two_index[0] == i:\n",
    "                word.append(j)\n",
    "            elif two_index[1] == i:\n",
    "                word.append(j)\n",
    "                words.append(word)\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for technology\n",
    "def get_tech_words(x):\n",
    "    get_all = []\n",
    "    for document in x:\n",
    "        sys_words = ['by', 'with', 'using']\n",
    "        get_all_words = []\n",
    "        for i in sys_words:\n",
    "            get_words = get_index(document, i)\n",
    "            get_all_words.extend(get_words)\n",
    "        get_all.extend(get_all_words)\n",
    "    return get_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_results_2 = preresults.apply(lambda x:lemm_noun(x))\n",
    "tech_words = get_tech_words(noun_results_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract only verb with stopwords\n",
    "def stopwords_noun(x):\n",
    "    stopped_tech = []\n",
    "    stop = stopwords.words('english')\n",
    "    stop.extend(['healthcare', 'say', 'be', 'health', 'company', 'patient', 'others', 'help'])\n",
    "    for words in x:\n",
    "        if not words[0] in stop:\n",
    "            stopped_tech.append(words[0])\n",
    "        else:\n",
    "            stopped_tech.append(words[1])\n",
    "            \n",
    "    pos_list = []\n",
    "    for word in stopped_tech:\n",
    "        pos_tagging = pos_tag([word])\n",
    "        if pos_tagging[0][1][:2] == 'NN':\n",
    "            pos_list.append(pos_tagging[0][0])\n",
    "    \n",
    "    return pos_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "tech_stopped = stopwords_noun(tech_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df_2 = make_tfidf_dataframe(tfidf_dict, tech_stopped)\n",
    "with open('tech_tfidf_df.pkl', 'wb') as f:\n",
    "    pickle.dump(tfidf_df_2, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
