{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import pos_tag\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import re, string, itertools\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2816\n"
     ]
    }
   ],
   "source": [
    "# read\n",
    "news_1 = pd.read_excel('healthcaremarket.xlsx')\n",
    "news_2 = pd.read_excel('healthcareitnews_add.xlsx')\n",
    "news_1 = news_1.iloc[447:,:]\n",
    "news_1 = news_1['text']\n",
    "news_2 = news_2['text']\n",
    "news = pd.concat([news_1, news_2])\n",
    "\n",
    "print(len(news))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "def pre_process_wordlist(txt):\n",
    "    txt = txt.strip()\n",
    "    txt = txt.lower()\n",
    "    txt = txt.replace('\\t', ' ')\n",
    "    txt = txt.replace('\\n', '')\n",
    "    txt = txt.replace('.', '. ')\n",
    "    #txt = re.sub('[^a-zA-Z0-9]',' ',txt)\n",
    "    txt = re.sub('[$]','',txt)\n",
    "    txt = re.sub('[–]','',txt)\n",
    "    txt = re.sub('[0-9]',' ',txt)\n",
    "    txt = txt.replace(u'\\xa0', u' ')\n",
    "    txt = txt.replace('  ', ' ')\n",
    "    txt = txt.replace('   ', ' ')\n",
    "    txt = txt.replace('  ', ' ')\n",
    "    txt = txt.replace(' .', '.')\n",
    "    txt = re.sub('[-=+,#/\\?:“”^\"—$€£@*\\\"※~&%ㆍ!』’\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》]', '', txt)\n",
    "    txt = txt.strip()\n",
    "    # 단어 길이 3개이하 삭제\n",
    "    # shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')\n",
    "    # txt = shortword.sub('', txt)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract two words after a specific word\n",
    "def get_index(doc, cword):\n",
    "    tokens = word_tokenize(doc)\n",
    "    words_index = []\n",
    "    \n",
    "    for i, j in enumerate(tokens):\n",
    "        \n",
    "        if j == cword:\n",
    "            word_index = [i+1, i+2]\n",
    "            words_index.append(word_index)\n",
    "            \n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    words = []\n",
    "    \n",
    "    for two_index in words_index:\n",
    "        word = []\n",
    "        \n",
    "        for i, j in enumerate(tokens):\n",
    "            \n",
    "            if two_index[0] == i:\n",
    "                word.append(j)\n",
    "                \n",
    "            elif two_index[1] == i:\n",
    "                word.append(j)\n",
    "                \n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "        words.append(word)\n",
    "            \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count most common word and make dataframe\n",
    "def counter_df(x):\n",
    "    counted_words = Counter(x)\n",
    "    x_counter = [] \n",
    "    \n",
    "    for i, j in counted_words.items():\n",
    "        x_counter.append(i)\n",
    "            \n",
    "    return x_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "preresults = news.apply(lambda x:pre_process_wordlist(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # System words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for system\n",
    "def get_system_words(x):\n",
    "    get_all = []\n",
    "    \n",
    "    for document in x:\n",
    "        \n",
    "        sys_words = ['for', 'to']\n",
    "        get_all_words = []\n",
    "        \n",
    "        for i in sys_words:\n",
    "            get_words = get_index(document, i)\n",
    "            get_all_words.extend(get_words)\n",
    "            \n",
    "        get_all.extend(get_all_words)\n",
    "        \n",
    "    return get_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_words = get_system_words(preresults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract only verb with stopwords\n",
    "def stopwords_verb(x):\n",
    "    stopped_sys = []\n",
    "    stop = stopwords.words('english')\n",
    "    stop.extend(['healthcare', 'say', 'be'])\n",
    "    \n",
    "    for words in x:\n",
    "        \n",
    "        if not words[0] in stop:\n",
    "            stopped_sys.append(words[0])\n",
    "            \n",
    "        else:\n",
    "            stopped_sys.append(words[1])\n",
    "    \n",
    "    pos_list = []\n",
    "    \n",
    "    for word in stopped_sys:\n",
    "        pos_tagging = pos_tag([word])\n",
    "        \n",
    "        if pos_tagging[0][1][:2] == 'VB':\n",
    "            pos_list.append(pos_tagging[0][0])\n",
    "            \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemma = [lemmatizer.lemmatize(i, pos='v') for i in pos_list]\n",
    "    \n",
    "    return lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopped_verb = stopwords_verb(system_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_seri = pd.Series(counter_df(stopped_verb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1011"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(system_seri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Technology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for technology\n",
    "def get_tech_words(x):\n",
    "    get_all = []\n",
    "    \n",
    "    for document in x:\n",
    "        sys_words = ['by', 'with', 'using']\n",
    "        get_all_words = []\n",
    "        \n",
    "        for i in sys_words:\n",
    "            get_words = get_index(document, i)\n",
    "            get_all_words.extend(get_words)\n",
    "            \n",
    "        get_all.extend(get_all_words)\n",
    "        \n",
    "    return get_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tech_words = get_tech_words(preresults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract only verb with stopwords\n",
    "def stopwords_noun(x):\n",
    "    stopped_tech = []\n",
    "    stop = stopwords.words('english')\n",
    "    stop.extend(['healthcare', 'say', 'be', 'health', 'company', 'patient', 'others', 'help'])\n",
    "    \n",
    "    for words in x:\n",
    "        \n",
    "        if not words[0] in stop:\n",
    "            stopped_tech.append(words[0])\n",
    "            \n",
    "        else:\n",
    "            stopped_tech.append(words[1])\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemma = [lemmatizer.lemmatize(i, pos='n') for i in stopped_tech]\n",
    "    \n",
    "    pos_list = []\n",
    "    \n",
    "    for word in lemma:\n",
    "        pos_tagging = pos_tag([word])\n",
    "        \n",
    "        if pos_tagging[0][1][:2] == 'NN':\n",
    "            pos_list.append(pos_tagging[0][0])\n",
    "    \n",
    "    return pos_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopped_noun = stopwords_noun(tech_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tech_seri = pd.Series(counter_df(stopped_noun))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3444"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tech_seri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # People"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract only noun with stopwords\n",
    "def people_noun(x):\n",
    "    stop = stopwords.words('english')\n",
    "    is_noun = lambda pos: pos[:2] == 'NN'\n",
    "    \n",
    "    noun_list = []\n",
    "    \n",
    "    for sentence in x:\n",
    "        tokens = word_tokenize(sentence)\n",
    "        noun = [word for (word, pos) in pos_tag(tokens) if is_noun(pos)]\n",
    "        noun_list.extend(noun)\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemma = [lemmatizer.lemmatize(i, pos='n') for i in noun_list]\n",
    "    \n",
    "    result = [i for i in lemma if not i in stop]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_tokens = people_noun(preresults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split corpus due to the fact that nlp's limit is 1,000,000\n",
    "all_corpus = []\n",
    "\n",
    "for i in range(0, len(people_tokens), 8000):\n",
    "    corpus = ' '.join(people_tokens[i:i+8000])\n",
    "    all_corpus.append(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NER dictionary in spacy\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "ner_dict = {}\n",
    "\n",
    "for i in all_corpus:\n",
    "    doc = nlp(i)\n",
    "    \n",
    "    for e in doc:\n",
    "        \n",
    "        if e.ent_type_ != \"\":\n",
    "            \n",
    "            if len(e) > 2:\n",
    "                \n",
    "                ner_dict[e] = e.ent_type_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "person = []\n",
    "norp = []\n",
    "org = []\n",
    "product = []\n",
    "\n",
    "for i, j in ner_dict.items():\n",
    "    \n",
    "    if j == 'PERSON':\n",
    "        person.append(str(i))\n",
    "        \n",
    "    elif j == 'NORP':\n",
    "        norp.append(str(i))\n",
    "        \n",
    "    elif j == 'ORG':\n",
    "        org.append(str(i))\n",
    "        \n",
    "    elif j == 'PRODUCT':\n",
    "        product.append(str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_seri = pd.Series(counter_df(product))\n",
    "norp_seri = pd.Series(counter_df(norp))\n",
    "org_seri = pd.Series(counter_df(org))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(831, 213, 7899)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(product_seri), len(norp_seri), len(org_seri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Make final dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1011 3444 831 213 7899\n"
     ]
    }
   ],
   "source": [
    "print(len(system_seri), len(tech_seri), len(product_seri), len(norp_seri), len(org_seri))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ppl_product</th>\n",
       "      <th>ppl_norp</th>\n",
       "      <th>ppl_org</th>\n",
       "      <th>system</th>\n",
       "      <th>technology</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>winwin</td>\n",
       "      <td>iam</td>\n",
       "      <td>intelligence</td>\n",
       "      <td>keep</td>\n",
       "      <td>fda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apache</td>\n",
       "      <td>stratifi</td>\n",
       "      <td>business</td>\n",
       "      <td>see</td>\n",
       "      <td>ai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>solr</td>\n",
       "      <td>saudi</td>\n",
       "      <td>pay</td>\n",
       "      <td>disrupt</td>\n",
       "      <td>safety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>salesforce</td>\n",
       "      <td>babylonian</td>\n",
       "      <td>university</td>\n",
       "      <td>improve</td>\n",
       "      <td>digital</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cytoflex</td>\n",
       "      <td>thingsiot</td>\n",
       "      <td>exeter</td>\n",
       "      <td>develop</td>\n",
       "      <td>aim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7894</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>leverage</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7895</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>marr</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7896</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>keypress</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7897</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>insideout</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7898</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>weider</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7899 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ppl_product    ppl_norp       ppl_org   system technology\n",
       "0         winwin         iam  intelligence     keep        fda\n",
       "1         apache    stratifi      business      see         ai\n",
       "2           solr       saudi           pay  disrupt     safety\n",
       "3     salesforce  babylonian    university  improve    digital\n",
       "4       cytoflex   thingsiot        exeter  develop        aim\n",
       "...          ...         ...           ...      ...        ...\n",
       "7894         NaN         NaN      leverage      NaN        NaN\n",
       "7895         NaN         NaN          marr      NaN        NaN\n",
       "7896         NaN         NaN      keypress      NaN        NaN\n",
       "7897         NaN         NaN     insideout      NaN        NaN\n",
       "7898         NaN         NaN        weider      NaN        NaN\n",
       "\n",
       "[7899 rows x 5 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge all dataframes\n",
    "morpho_df = pd.concat([product_seri, norp_seri, org_seri, system_seri, tech_seri], axis=1)\n",
    "morpho_df.columns = ['ppl_product', 'ppl_norp', 'ppl_org', 'system', 'technology']\n",
    "morpho_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "morpho_df.to_excel('morpho_df.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
